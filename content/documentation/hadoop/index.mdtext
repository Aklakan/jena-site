Title: RDF Tools for Apache Hadoop

RDF Tools for Apache Hadoop is a set of libraries which provide various basic building blocks which enable
you to start writing Hadoop based applications which work with RDF data.

Historically there has been no serious support for RDF within the Hadoop ecosystem and what support has existed has
often been limited and task specific.  These libraries aim to be as generic as possible and provide the necessary
infrastructure that enables developers to create their application specific logic without worrying about the
underlying plumbing.

## Documentation

- [Overview](#overview)
- [Getting Started](#getting-started)
- APIs
    - [Common](common.html)
    - [IO](io.html)
    - [Map/Reduce](mapred.html)
- Examples
    - [RDF Stats Demo](demo.html)
- [Maven Artifacts for Jena JDBC](artifacts.html)

## Overview

RDF Tools for Apache Hadoop is published as a set of Maven module via its [maven artifacts](artifacts.html).  The source for this libraries
may be [downloaded](/download/index.cgi) as part of the source distribution.  These modules are built against the Hadoop 2.x. APIs and no
backwards compatibility for 1.x is provided.

The core aim of these libraries it to provide the basic building blocks that allow users to start writing Hadoop applications that
work with RDF.  They are mostly fairly low level components but they are designed to be used as building blocks to help users and developers
focus on actual application logic rather than on the low level plumbing.

Firstly at the lowest level they provide `Writable` implementations that allow the basic RDF primitives - nodes, triples and quads -
to be represented and exchanged within Hadoop applications, this support is provided by the [Common](common.html) library.

Secondly they provide support for all the RDF serialisations which Jena supports as both input and output formats subject to the specific 
limitations of those serialisations.  This support is provided by the [IO](io.html) library in the form of standard `InputFormat` and
`OutputFormat` implementations.

There are also a set of basic `Mapper` and `Reducer` implementations provided by the [Map/Reduce](mapred.html) library which contains code
that enables various common Hadoop tasks such as counting, filtering, splitting and grouping to be carried out on RDF data.  Typically these
will be used as a starting point to build more complex RDF processing applications.

Finally there is a [RDF Stats Demo](demo.html) which is a runnable Hadoop job JAR file that demonstrates using these libraries to calculate
a number of basic statistics over arbitrary RDF data.

## Getting Started

To get started you will need to add the relevant dependencies to your project, the exact dependencies necessary will depend 
on what you are trying to do.  Typically you will likely need at least the IO library and possibly the Map/Reduce library:

    <dependency>
      <groupId>org.apache.jena</groupId>
      <artifactId>jena-hadoop-rdf-io</artifactId>
      <version>x.y.z</version>
    </dependency>
    <dependency>
      <groupId>org.apache.jena</groupId>
      <artifactId>jena-hadoop-rdf-mapreduce</artifactId>
      <version>x.y.z</version>
    </dependency>

## APIs

There are three main libraries each with their own API:

- [Common](common.html) - this provides the basic data model for representing RDF data within Hadoop
- [IO](io.html) - this provides support for reading and writing RDF
- [Map/Reduce](mapred.html) - this provides support for writing Map/Reduce jobs that work with RDF



